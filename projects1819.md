## Assignment

| Student   |      Master Thesis      |  Advisors |
|----------|:-------------:|------:|
| Núria Valls, Albert Prat	 |  Particle Swarm Optimization | Gerard Gomez |
| Marco Remane |	IOMed	| Santi Seguí |
|  Daniel Soler	| Nostrum Biodiscovery	| Jordi Vitrià  |
|  Ramon Mir Mora, Sergi Sánchez de la Blanca Contreras, Philippe van Amerongen	| Agent-based models for assessing the risk of default propagation in interconnected sectorial financial networks.	| Jordi Nin  |
|  Tom Rolandus | Hagedoorn	Machine learning algorithms for detecting re-identification risks in anonymized data. |	Francesco Bonchi (Tutor: Jordi Vitrià)  |
|  Cristina Rosich Solé, Adrian Dueñas Pelaez	| Uncovering the role of airborne microbes in Kawasaki Disease.	| Xavier Rodó (ISGlobal) and Santi Seguí  |
|  Rubén Barco Terrones, Aleix Casellas Comas, Andreu Masdeu Ninot, Pablo Lázaro Herrasti	| Multi-modal group behavior and interaction analysis.	| Sergio Escalera  |
|  Gerard Marrugat	| Applying Deep Learning for Food image Analysis.	| Petia Radeva  |
|  Èlia Ficapal Vila, Stefan Ivanov	| Endoluminal image classification	| Santi Seguí  |
|  Spela Zavodnik, Alejandro Castelo	| Data Science to expand the bioactive chemical space.	| IRB & Jordi Vitrià  |
|  Peter Weber, Eduard Ribas	| Man-made structures detection from space.	| Satellogic &  Jordi Vitrià |
| Alberto Huélamo-Segura | Tile embedding | Satellogic & Jordi Vitrià|






## Agent-based models for assessing the risk of default propagation in interconnected sectorial financial networks

Advisor: Jordi Nin (jordi.nin@gmail.com)

Description: 
Interconnected sectorial financial networks are the substrate for economic agents. One of the main questions nowadays is to understand how systemic risk can raise in financial network. Systemic risk is understood as the probability of large cascades of entangled economic events triggered by causes that range from exogenous shocks to endogenous defaults. The succession of defaults can jeopardize the full system. The interplay between the topology of the underlying interaction network and the easiness of events to propagate has proven to be essential to understand the proportion of the financial system affected by default avalanches and to assess the systemic risk. Avalanches in financial systems are understood as dynamical processes that correlate individual economic states of the agents when a triggering stress event materializes.
In this project students should design a set of different autonomous agents able to interact in a financial network. Then, perform a set of experiments in a simulated sectorial financial network to find the most efficient behavior when an economic crisis raise in a financial network

The project will be developed by 3 students with the following tasks:
- Network data generation (Student 1)
- Agent-based model implementations (min 1 model, ideally 3) (Students 1-3)
- Simulation execution (Student 2)
- Results analysis (Student 3)
- Report writting  (Student 1-3)
- Optional: Scientific Publication  (Student 1-3)


## Endoluminal image classification.
 
Advisor: Santi Seguí (santi.segui@ub.edu)

Capsule endoscopy is a procedure that uses a tiny wireless camera to take pictures of your digestive tract. A wireless capsule endoscopy (WCE) camera sits inside a vitamin-size capsule you swallow and takes more than 50.000 images which are sent to an external device in order to be analyzed. 

Although this is an amazing non-invasive product that allows the full visualization of the entire endoluminal track, its application is limited due to one main problem: the diagnosis, visualization of more than 60.000 images, is a hard and tedious task that must be done by experts. So, WCE needs AI to become a real clinical procedure.

In this project, the student will have to develop deep learning models to find several dysfunctions (e.g. polyps, blood, ulcers) in those images using machine learning and deep learning models. 

Practical Information:

+	This is an individual project.
+	This project will be developed in collaboration with the company Corporate Health and other students and researchers from the University of Barcelona.
+	Possibility to do the Master’s Thesis Project as a part of a paid research grant depending on the CV and availability.
+	Possibility to continue the project after the end Master’s Thesis with a 3 years PhD grant


## Machine learning algorithms for detecting re-identification risks in anonymized data

Supervision: Dr. Francesco Bonchi (www.francescobonchi.com)
Period: January - June 2019
Salary: TBD

The work to be developed during the internship (or Master thesis), is part
of Eurecat’s contribution to the European project called SMOOTH
(https://smoothplatform.eu/), which deals with compliance of the new General
Data Protection Regulation (GDPR).
More background information on SMOOTH project can be provided upon request.

The specific task that the student will tackle, requires to develop methods
for detecting re-identification risks in an anonymized database, such as the
number of unique tuples in the database after removal of obvious personal
identifiers and quasi-identifiers, as well as to determine data minimization
strategies, i.e., to compare the attributes that are needed for the business
with the attributes that are actually found in that business’ forms and
repositories.

The project is at the intersection between applied data science and more
fundamental research. It requires identifying articles in the scientific
literature, as well as pre-existing software libraries, that can help in our
task. The project will involve some coding, prototyping and experimenting.
It is indicated for a motivated student  with a good Data Science background
wishing to tackle bold challenges, using innovative thinking and
problem-solving skills in a research project with concrete, real-world
deployment.

Practical Information:

+ This is an individual project.
+ This project will be developed in Eurecat, Barcelona (www.eurecat.org)


## Explainable systems for place characterization

Advisor: Mireia Ribera (ribera@ub.edu) & Àgata Lapedriza (alapedriza@uoc.edu)

2 students

Description: Automatic place characterization is an important task in computer vision to make machines understand the environments . The goal of automatic place characterization is to describe characteristics of a place shown in an image or a video. While most of the efforts in making explainable systems for scene understanding have been focused in place categorization, action recognition, and object detection, there are no many efforts in explaining the characteristics of a place: two different instances of place of the same category (for instance two different bedrooms) can be very different (more or less wide, with natural or artificial lighting, with an office space or without it, etc). 

The goal of this project is to develop an explainable Deep Learning method that recognizes space attributes and gives visual explanations (heat maps) of the place attributes found. 

Main tasks involve:

- Read related work on Place categorization, place attribute recognition and Class Activation Maps (CAM) for visual explanations.
- Read related work about explainability and evaluation of explainability
- Adaptation of Places CAM code for scene recognition to the recognition of place attributes.
- Select how to measure the explanation effectiveness and with which kind of users: developers; domain experts or final users
- Test the adapted method in terms of performance 
- Evaluate the explainability according to the selected criteria

For more information about the techniques that are going to be used in this project you can try our Place Recognition demo:

http://places2.csail.mit.edu/index.html
http://cnnlocalization.csail.mit.edu

References:

Zhou et al “Places: a 10 million image database for scene recognition”  DOI: 10.1109/TPAMI.2017.2723009 http://places2.csail.mit.edu/PAMI_places.pdf 
Patterson et al “The SUN attribute database” Int J. Comput Vis (2014) 108:59-81 http://cs.brown.edu/~gmpatter/pub_papers/sun_attributes_ijcv.pdf 
Zhou et al. “Learning deep features for discriminative Localization” CVPR  http://people.csail.mit.edu/bzhou/publication/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf
David Gunning “Explainable Artificial Intelligence (XAI)” DARPA 2017
https://www.darpa.mil/attachments/XAIProgramUpdate.pdf
Gilpin et al “Explaining  explanations” (2018) https://arxiv.org/abs/1806.00069 
This project is coadvised by Mireia Ribera (UB) and Agata Lapedriza (UOC-MIT).


## Man-made structures detection from space.

Advisor: Jordi Vitrià (jordi.vitria@ub.edu)

With the development of affordable and recurrent remote sensing technology, we can now access frequent geospatial information in different levels of detail, ranging from 100m to 0.01m.  The task of detecting various types of man-made structure and man-induced change has become a key problem in remote sensing image analysis. 

This project is focused on providing an answer to a simple question: what is the minimum resolution required to detect the different man-made structures (roads, buildings, cars, pipes, crop fields, etc.) in remote sensing images? Determining this value is important not only for designing optimal satellite sensors (in terms of cost vs image information) but also to use optimal data sources (also in terms of cost vs image information) when developing data-based remote sensing products. At a global level, this knowledge contributes to understanding the impact of our species on the planet.

The approach will be based on the analysis of the statistical properties of remote sensing images. There are two expected outcomes of the analysis: (i) the definition of an image signature to discriminate between natural and different man-made structures and (ii) experiments showing which is the optimal resolution.    

The study of the statistical properties of natural images belonging to different categories and their relevance for scene and object categorization tasks has been an active field of research for several decades. Results show how visual categorization based directly on low-level features, without grouping or segmentation stages, can benefit object localization and identification. In this project, we want to extend this study to remote sensing images belonging to man-made and natural structures. Taking for granted that detection methods will be based on deep learning, we will use deep learning low-level detectors (extracted from convolutional neural networks) to define image signatures.

Practical Information:

+ This is an individual project.
+ This project will be developed in collaboration with Satellogic (https://www.satellogic.com/), a space technology company that develops micro-satellites to provide hyperspectral earth imaging and geo-information analytics. 
+ Tasks: 1) To study deep learning detection methods / 2) Develop an image signature based on deep learning low-level features / 3) Design and implement an empirical demonstration of the project's hypothesis.


## Data Science to expand the bioactive chemical space

Advisor: Patrick Aloy (IRB), Jordi Vitrià (jordi.vitria@ub.edu)

Biological data is accumulating at an unprecedented rate, escalating the role of data-
driven methods in computational drug discovery. The urge to couple biological data to
cutting-edge machine learning has spurred developments in data integration and
knowledge representation, especially in the form of heterogeneous, multiplex and
semantically-rich biological networks. 

Today, thanks to the propitious rise in knowledge embedding techniques, 
these large and complex biological networks can be converted
to a vector format that suits the majority of machine learning implementations. Indeed,
we have generated biological embeddings, or bio-prints, that capture complex
relationships between small molecules and other biological entities such as targets or
diseases. However, only a tiny fraction of the possible chemical space has been so far
explored, meaning that most compounds able to modulate biological activities (i.e.
drugs) are yet to be discovered. Accordingly, the main objective of this project is to
couple our bio-prints to inverse design algorithms to generate new chemical entities with
the desired functionality. For instance, variational autoencoders or GANs are well suited
to learn embeddings by simply reading the SMILES strings that are stored in large
compound repositories and reversibly generate novel and valid SMILES strings,
optimized for a certain property of interest, through the trained functions. All in all, the
incorporation of machine learning methods, able to consider huge amounts of
(individual) biological data, to the drug discovery process will trigger the development of
thousands of novel compounds, finally enabling precision medicine.

Practical Information:

+ This is an individual project.
+ This project will be developed in collaboration with IRB (https://www.irbbarcelona.org/). 
+ Tasks: 1) To study the application of deep learning methods to biology/ 2) Develop a Generative Adversarial Network (GAN) for SMILE data / 3) Test the approach for drug discovery.



## Large-Scale Support Vector Machine Learning using data statistical description

Advisor: Lluís Garrido (lluis.garrido@ub.edu) & Arturo Vieiro (vieiro@ub.edu)

The Support Vector Machine (SVM) is a widely used kernel learning algorithm. It
is able to achieve robust pattern recognition using well established concepts
in optimization theory. The implementation of efficient SVM solvers has
diverged from the classical methods of numerical optimization. Whereas
numerical optimization literature focuses on arriving quickly to an accurate
solution, machine learning algorithms often use lower iteration complexity
algorithms. In addition, large-scale learning algorithms are constrained by the
total computational resources. In this work we propose to investigate the
possibility of developing a SVM learning method for high dimensional data based
on previously reducing the dimensionality of the data set. This would allow us
to apply methods suitable for lower dimensional scale problems. The
dimensionality reduction can be achieved by substituting the original data set
by data (parameters) of its statistical description.  This reduction can be
implemented as a tree algorithm. For the reduced data set the learning can be
performed in the dual space.

TFM for one student.


## Automatic Document Reader Platform
 
Advisor: Accenture (contact: jordi.vitria@ub.edu)

Description:
The aim is to build a frontend (visualization in Shiny) and backend (analytics tool in Python) to enable fast prototyping of ad-hoc solutions of document reading (OCR, Optical Character Recognition).
The goal is to digitize relevant contents from scanned documents, both in handwritten and typewritten format.
One will start from existing use cases already developed in our center, that include a backend both open-source OCR solutions and our own deep learning OCR code, and also an existing visualization tool.
New open-source codes in the field of RNN Deep Learning nets would be leveraged as well.

Practical Information:
+ This is a project for a team of 2 persons.
+ This project will be developed in Accenture, Barcelona

## Application of variants of Long Short-Term Memory (LSTM) algorithms to high resolution spectral time series.

Advisor: Accenture (contact: jordi.vitria@ub.edu)

Description:
LSTM and Nested LSTMs, as well as other types of Recurrent Neural Networks, can deal with processes occurring at arbitrary time scales which is the main requirement in industrial areas like Predictive Asset Maintenance.
It would be challenging to study and assess whether temporal hierarchies self-learned by the model perform better that knowledge-based prescribed time scales. Analysis should provide new insights based on the robustness of the methodological approach in terms of computing performance and accuracy.
We propose the candidate to explore state-of-the-art new techniques to identify best methodological approach for predicting failures in rotating equipment.
References:
- Moniz, J.R.A & D. Krueger. (2017). Nested LSTMs. Proceeding in Machine Learning Research 77: 530-544. http://proceedings.mlr.press/v77/moniz17a/moniz17a.pdf
- Rupesh, K.G., K. Srivastava, J. Koutník, B.R. Steunebrink and J. Schmidhuber. (2017). LSTM: A Search Space Odyssey. Transactions on Neural Networks and Learning Systems. https://arxiv.org/pdf/1503.04069

Practical Information:
+ This is an individual project.
+ This project will be developed in Accenture, Barcelona

## Hybrid application approach of sonic and vibrational techniques for early failure detection in pumps and high rotation speed engines

Advisor: Accenture (contact: jordi.vitria@ub.edu)

Description:
Advanced spectral analysis techniques allow reliability engineers to precisely identify quite a large number of failure modes in rotating engines (motors, pumps, gearboxes, turbines, etc.). However, the different types of sensors are not equally suited for all failure modes, ranges of analysis and equipment, and new insights from a Data Science perspective are needed for discovering new analytical approaches, especially when high frequency ranges of analysis are required.
References:
- Hussain, S. & H.A. Gabbar. (2013). Vibration Analysis and Time Series Prediction for Wind Turbine Gearbox Prognostics. Int. J. Prognostics and Health Management,  ISSN 2153-2648, 2013 014. http://ftp.phmsociety.org/sites/phmsociety.org/files/phm_submission/2013/ijphm_13_014.pdf
- Albraik, A., F. Althobiani, F. Gu and A. Ball. (2012). Diagnosis of centrifugal pump faults using vibration methods. J. Physics, Conf. Series 364: 012139. https://www.researchgate.net/publication/254495843_Diagnosis_of_Centrifugal_Pump_Faults_Using_Vibration_Methods
- Cernetic, J. (2009). The use of noise and vibration signals for detecting cavitation in kinetic pumps. J. Mech, Eng. Sci., Proc. IMechE. Vol. 223, Part C.: 1645-1655. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1022.8718&rep=rep1&type=pdf___

Practical Information:
+ This is an individual project.
+ This project will be developed in Accenture, Barcelona


## Uncovering the role of airborne microbes in Kawasaki Disease

Advisor: Xavier Rodó (ISGlobal) and Santi Seguí (santi.segui@ub.edu)

Rationale: Kawasaki disease (KD) is a long known self-limited vasculitis of unknown origin that affects children worldwide, and that may have serious consequences if left untreated. It was discovered by Dr. Kawasaki in the 1960s and genètic susceptibility has been claimed as an underlying predisposing factor. However, in the last few years a consortium led by the ISGlobal Climate & Health Program has set the basis for a revolutionary hypothesis pointing to an airborne cause of this mysterious syndrome.
The current proposal seeks to test whether air quality, in particular bacteria and fungi, can lie at the basis of KD etiology. A weekly record of unprecedented air samples taken in Japan in 2014 and 2015, covering 1,5 years has been fully analyzed for its biological composition and air chemistry. The student will have at hand an extense data base of over 2000 species in total and the objective is to check what relationships there are among individual or clústers of species, with KD incidence. To accomplish these objectives, it is expected that the datasets can be analyzed with a variety of classification, ordination and other data statistical techniques.
This project will be develop by a group of 1-3 students in ISGlobal.


## What data analyses can tell us about the Dynamics and etiology of Kawasaki disease in Japan?

Advisor: Xavier Rodó (ISGlobal) and Santi Seguí (santi.segui@ub.edu)

Rationale: Kawasaki disease (KD) is a long known self-limited vasculitis of unknown origin that affects children worldwide, and that may have serious consequences if left untreated. It was discovered by Dr. Kawasaki in the 1960s and genètic susceptibility has been claimed as an underlying predisposing factor. However, in the last few years a consortium led by the ISGlobal Climate & Health Program has set the basis for a revolutionary hypothesis pointing to an airborne cause of this mysterious syndrome.
The current proposal seeks to uncover what differences there are in KD population epidemiology among the 47 prefectures in Japan for an unprecedented record of daily incidence covering the interval 1970-2016. Timeseries analyses and spatial classification and data ordination techniques will be needed to explain what patterns in both space and time the disease epidemiology describes, provided there are known spatial (e.g. North-South of the Japanese archipelago) and temporal differences (changes among the seasonal paterns in distant prefectures, delays among the three main epidèmics among diferent prefectures, etc...) The candidate will have to work also with both atmospheric and aerosol time series of data at the scale of the same prefectures in the search for consistent covarying environmental-disease dynamics.
This project will be develop by a group of 1-3 students in ISGlobal.


## Multi-modal group behavior and interaction analysis 

Advisor: Sergio Escalera (sescalera@ub.edu)

The study of how human beings react to the environment and interact with it and each other is an ongoing research line in the computer vision and machine learning communities. Domains such as social signal processing and affective computing aim at understanding and modeling interactions by extracting audio-visual social signals and correlate them to bigger constructs such as dominance or personality, among others. 

The goal of this project is to explore and analyze the individual behaviors and interaction dynamics of a group of people while interacting with each other. The interaction sessions are recorded using a multi-camera calibrated setup, and are divided in two scenarios: dyadic and group interactions. Behavioral cues will be extracted using computer vision and deep learning techniques, and then correlated with other modalities, such as electroencephalogram (EEG) data, Big Five personality tests, and sociodemographic questionnaires.

Main tasks involve:
•	Data synchronization and preprocessing.  
•	Individual social signals and behavior extraction from video streams. We will focus on emotion from facial expressions, eye gaze estimation and pose analysis.
•	Fusion of extracted social signals and behaviors.
•	Analysis of interaction dynamics.
•	Statistical correlation to other modalities.

The project is intended for three to five students, but other groupings and the concrete tasks will be discussed together with the project supervisors.
 
References:
1.	Vinciarelli, Alessandro, Maja Pantic, and Hervé Bourlard. "Social signal processing: Survey of an emerging domain." Image and vision computing 27.12 (2009): 1743-1759
2.	Escalera, Sergio, et al. "Guest Editorial: Apparent Personality Analysis." IEEE Transactions on Affective Computing 3 (2018): 299-302.
3. Escalera, Sergio, et al. "Guest Editorial: The Computational Face." IEEE Transactions on Pattern Analysis & Machine Intelligence 11 (2018): 2541-2545.


## Video Summarization personalized to Visual Disabilities

Advisor: Sergio Escalera (sescalera@ub.edu) & Mireia Ribera (ribera@ub.edu)

Video summarization aims at summarizing most relevant information from a video just in few “keyframes”. Applications are countless, including video indexing and retrieval, TV news summarization, sport summarization, video summarization in medical treatments, or risk event detection in health and surveillance scenarios, just to mention a few.  Video is becoming the most prominent media format in many contexts.

While the task has been widely explored in the computer vision and machine learning communities during several years, still many challenges remain unsolved: how can we evaluate the quality of a video summary? is there a good trade-off between compact representation versus maximized video understanding? does video summarization target all audiences? how can we customize video summaries to visual disabilities?

This last question is the main objective of this master thesis. While we will analyze current deep learning strategies and compare them to maximize the information we can provide with few frames, we will integrate summarization requirements based on the needs of people with different visual disabilities (glaucoma, diabetic retinopathy, macular degeneration, color deficiency). Therefore, summarization will be personalized for the different user profiles, both at the stage of information selection and summarization, and during information display (cropping, resolution, recoloring, etc.). 

Main tasks involve:
-	the analysis and evaluation of current state of the art of deep learning video summarization strategies, focusing on sport datasets 
-	Summarization metrics will be defined, 
-	Different deep learning summarization strategies will be described
-	adaptation of existing summarization strategies based on the needs of different visual disabilities in order to maximize the interpretability by the user. This will require spatial and temporal semantic video description 
-	adaptation of the final output to the different profile requirements, particularly color defficiency profiles. Analysis of techniques for recoloring or adding patterns to the final selected key-frames
-	creation of a proof of concept of video summary based on a sport dataset

The project is intended for three or four students, but other groupings and the concrete tasks could be discussed together with this project supervisors, Sergio Escalera and Mireia Ribera

References

1.	Hsuan-I Ho, Wei-Chen Chiu, Yu-Chiang Frank Wang, Summarizing First-Person Videos from Third Persons' Points of Views, European Conference on Computer Vision, ECCV, 2018
2.	Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman, Video Summarization with Long Short-term Memory, European Conference on Computer Vision, ECCV, 2016
3.	Mayu Otani, Yuta Nakashima, Naokazu Yokoya, Video Summarization using Deep Semantic Features, Asian Conference on Computer Vision, ACCV, 2016
4.	Chu WT., Yang TH. (2017) A Study of Combining Re-coloring and Adding Patterns to Images for Dichromats. In: Chen CS., Lu J., Ma KK. (eds) Computer Vision – ACCV 2016 Workshops. ACCV 2016. Lecture Notes in Computer Science, vol 10116. Springer, Cham
5.	Westman S. (2010) Evaluation Constructs for Visual Video Summaries. In: Lalmas M., Jose J., Rauber A., Sebastiani F., Frommholz I. (eds) Research and Advanced Technology for Digital Libraries. ECDL 2010. Lecture Notes in Computer Science, vol 6273. Springer, Berlin, Heidelberg
6.	Kovalev V., Petrou M. (2005) Optimising the Choice of Colours of an Image Database for Dichromats. In: Perner P., Imiya A. (eds) Machine Learning and Data Mining in Pattern Recognition. MLDM 2005. Lecture Notes in Computer Science, vol 3587. Springer, Berlin, Heidelberg



## Applyig Deep Learning to Analyze Lifestyle in Egocentric images

Advisor: Petia Radeva (radevap@gmail.com, Petia.ivanova@ub.edu)

Description
 
Egocentric images are acquired by a wearable camera that is able to take pictures during long periods of time. From egocentric images we can observe physicial activities of persons wearing the camera as shopping, jogging, socializing, etc. The purpose is to apply Deep learning techniques to analyze and recognize physical activities of persons wearing the camera.

According to the advances of the Master project, we will collaborate with psychologists from Consorci Sanitari de Terrassa that use egocentric images in order to create visual exercises for patients with mild cognitive impairment. 
Minim requirements 
 
Computational Vision, Machine learning, Python


## Applying Deep Learning for Food image Analysis

Advisor: Petia Radeva (radevap@gmail.com, Petia.ivanova@ub.edu)

Description
 
Today, Deep learning achieved outstanding results surpassing humans in many tasks as object recognition, lip reading, facial recognition, license plate readers, traffic violations detection, or breast tomosynthesis diagnosis. However, the food image analysis field is far unexplored. In this project, the Master student will explore Deep learning techniques to process food images and textual data (recipes, comments, etc.) in order to address problems as food recognition or food segmentation. We will explore new techniques for domain adaptation to transfer knowledge from different domains and improve food analysis.

According to the results, the Master thesis will be integrated in an European project where we collaborate with several European teams to validate our algorithms in real situation, like analyzing nutrition of patients with kidney transplant.

Minim requirements 
 
Computational Vision, Machine learning, Python



